{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-26T16:30:11.299274Z",
     "start_time": "2025-12-26T16:30:03.820336Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# 1. Setup paths and device\n",
    "local_model_path = \"../models\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 2. Load Model and Tokenizer\n",
    "print(f\"Loading model from {local_model_path}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(local_model_path)\n",
    "\n",
    "# Move model to GPU if available for speed\n",
    "model.to(device)\n",
    "\n",
    "# 3. Your Input Text\n",
    "article_text = \"\"\"\n",
    "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\n",
    "It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\n",
    "Locally nicknamed \"La Dame de Fer\" (French for \"The Iron Lady\"), it was constructed from 1887 to 1889\n",
    "as the centerpiece of the 1889 World's Fair. It has become a global cultural icon of France\n",
    "and one of the most recognizable structures in the world. The tower is 330 metres (1,083 ft) tall,\n",
    "about the same height as an 81-storey building, and the tallest structure in Paris.\n",
    "\"\"\"\n",
    "\n",
    "# 4. Tokenize (Pre-processing)\n",
    "# We must truncate long texts because BART has a limit (usually 1024 tokens)\n",
    "inputs = tokenizer(\n",
    "    article_text,\n",
    "    max_length=1024,   # BART's limit\n",
    "    truncation=True,   # Cut off text if it's too long\n",
    "    return_tensors=\"pt\" # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "# 5. Generate Summary (Inference)\n",
    "# This is where the magic happens. We don't just run model(), we run model.generate()\n",
    "summary_ids = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    num_beams=4,       # Beam search makes the summary higher quality\n",
    "    min_length=30,     # Force it to be at least this long\n",
    "    max_length=100,    # Cap the summary length\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# 6. Decode (Post-processing)\n",
    "# Convert the output numbers back into words\n",
    "summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"--- Original ---\")\n",
    "print(article_text)\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(summary_text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../models...\n",
      "--- Original ---\n",
      "\n",
      "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. \n",
      "It is named after the engineer Gustave Eiffel, whose company designed and built the tower. \n",
      "Locally nicknamed \"La Dame de Fer\" (French for \"The Iron Lady\"), it was constructed from 1887 to 1889 \n",
      "as the centerpiece of the 1889 World's Fair. It has become a global cultural icon of France \n",
      "and one of the most recognizable structures in the world. The tower is 330 metres (1,083 ft) tall, \n",
      "about the same height as an 81-storey building, and the tallest structure in Paris. \n",
      "\n",
      "\n",
      "--- Summary ---\n",
      "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It was constructed from 1887 to 1889 as the centerpiece of the 1889 World's Fair. The tower is 330 metres (1,083 ft) tall, about the same height as an 81-storey building.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"Falconsai/text_summarization\",\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Falconsai/text_summarization\"\n",
    ")\n",
    "input_text = \"\"\"Plants are remarkable organisms that produce their own food using a method called photosynthesis.\n",
    "This process involves converting sunlight, carbon dioxide, and water into glucose, which provides energy for growth.\n",
    "Plants play a crucial role in sustaining life on Earth by generating oxygen and serving as the foundation of most ecosystems.\"\"\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output = model.generate(**input_ids, cache_implementation=\"static\")\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ],
   "id": "55f0dfd85924a400",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T19:24:09.627182Z",
     "start_time": "2025-12-26T19:24:05.182348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# 1. Setup the device (Check for GPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_name = \"Falconsai/text_summarization\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 2. Move the Model to the device\n",
    "# We chain .to(device) immediately after loading\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "ARTICLE = \"\"\"\n",
    "Hugging Face: Revolutionizing Natural Language Processing\n",
    "Introduction\n",
    "In the rapidly evolving field of Natural Language Processing (NLP), Hugging Face has emerged as a prominent and innovative force. This article will explore the story and significance of Hugging Face, a company that has made remarkable contributions to NLP and AI as a whole. From its inception to its role in democratizing AI, Hugging Face has left an indelible mark on the industry.\n",
    "The Birth of Hugging Face\n",
    "Hugging Face was founded in 2016 by Cl√©ment Delangue, Julien Chaumond, and Thomas Wolf. The name \"Hugging Face\" was chosen to reflect the company's mission of making AI models more accessible and friendly to humans, much like a comforting hug. Initially, they began as a chatbot company but later shifted their focus to NLP, driven by their belief in the transformative potential of this technology.\n",
    "Transformative Innovations\n",
    "Hugging Face is best known for its open-source contributions, particularly the \"Transformers\" library. This library has become the de facto standard for NLP and enables researchers, developers, and organizations to easily access and utilize state-of-the-art pre-trained language models, such as BERT, GPT-3, and more. These models have countless applications, from chatbots and virtual assistants to language translation and sentiment analysis.\n",
    "Key Contributions:\n",
    "1. **Transformers Library:** The Transformers library provides a unified interface for more than 50 pre-trained models, simplifying the development of NLP applications. It allows users to fine-tune these models for specific tasks, making it accessible to a wider audience.\n",
    "2. **Model Hub:** Hugging Face's Model Hub is a treasure trove of pre-trained models, making it simple for anyone to access, experiment with, and fine-tune models. Researchers and developers around the world can collaborate and share their models through this platform.\n",
    "3. **Hugging Face Transformers Community:** Hugging Face has fostered a vibrant online community where developers, researchers, and AI enthusiasts can share their knowledge, code, and insights. This collaborative spirit has accelerated the growth of NLP.\n",
    "Democratizing AI\n",
    "Hugging Face's most significant impact has been the democratization of AI and NLP. Their commitment to open-source development has made powerful AI models accessible to individuals, startups, and established organizations. This approach contrasts with the traditional proprietary AI model market, which often limits access to those with substantial resources.\n",
    "By providing open-source models and tools, Hugging Face has empowered a diverse array of users to innovate and create their own NLP applications. This shift has fostered inclusivity, allowing a broader range of voices to contribute to AI research and development.\n",
    "Industry Adoption\n",
    "The success and impact of Hugging Face are evident in its widespread adoption. Numerous companies and institutions, from startups to tech giants, leverage Hugging Face's technology for their AI applications. This includes industries as varied as healthcare, finance, and entertainment, showcasing the versatility of NLP and Hugging Face's contributions.\n",
    "Future Directions\n",
    "Hugging Face's journey is far from over. As of my last knowledge update in September 2021, the company was actively pursuing research into ethical AI, bias reduction in models, and more. Given their track record of innovation and commitment to the AI community, it is likely that they will continue to lead in ethical AI development and promote responsible use of NLP technologies.\n",
    "Conclusion\n",
    "Hugging Face's story is one of transformation, collaboration, and empowerment. Their open-source contributions have reshaped the NLP landscape and democratized access to AI. As they continue to push the boundaries of AI research, we can expect Hugging Face to remain at the forefront of innovation, contributing to a more inclusive and ethical AI future. Their journey reminds us that the power of open-source collaboration can lead to groundbreaking advancements in technology and bring AI within the reach of many.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(ARTICLE, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "# 3. Move the Data (Inputs) to the device\n",
    "# We move the input_ids tensor to the GPU before passing it to the model\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "summary_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=1000,\n",
    "    min_length=30,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "# Decoding is usually done on CPU, so we don't need to move anything back manually\n",
    "# The tokenizer handles the device transfer internally during decoding if necessary\n",
    "summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print([{'summary_text': summary_text}])"
   ],
   "id": "915d74e84239fd5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "[{'summary_text': 'and AI as a whole Hugging Face has emerged as a prominent and innovative force. This article will explore the story and significance of Hugging Face. From its inception to its role in democratizing AI, Hugging Face has left an indelible mark on the industry. This article will explore the story and significance of Hugging Face Hugging Face. The Birth of Hugging Face Hugging Face Hugging Face is best known for its open-source contributions, particularly the \"Transformers Library:** The Transformers library provides'}]\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
